<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>proxygen: proxygen/folly/folly/container/F14.md Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">proxygen
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li class="current"><a href="files.html"><span>Files</span></a></li>
      <li><a href="examples.html"><span>Examples</span></a></li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="files.html"><span>File&#160;List</span></a></li>
      <li><a href="globals.html"><span>File&#160;Members</span></a></li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('F14_8md.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">proxygen/folly/folly/container/F14.md</div>  </div>
</div><!--header-->
<div class="contents">
<a href="F14_8md.html">Go to the documentation of this file.</a><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;# F14 Hash Table</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;F14 is a 14-way probing hash table that resolves collisions by double</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;hashing.  Up to 14 keys are stored in a chunk at a single hash table</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;position.  Vector instructions (SSE2 on x86_64, NEON on aarch64)</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;are used to filter within a chunk; intra-chunk search takes only a</div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;handful of instructions.  **F14** refers to the fact that the algorithm</div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;**F**ilters up to **14** keys at a time.  This strategy allows the hash</div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;table to be operated at a high maximum load factor (12/14) while still</div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;keeping probe chains very short.</div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;</div><div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;F14 provides compelling replacements for most of the hash tables we use in</div><div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;production at Facebook.  Switching to it can improve memory efficiency</div><div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;and performance at the same time.  The hash table implementations</div><div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;widely deployed in C++ at Facebook exist along a spectrum of space/time</div><div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;tradeoffs.  The fastest is the least memory efficient, and the most</div><div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;memory efficient (google::sparse_hash_map) is much slower than the rest.</div><div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;F14 moves the curve, simultaneously improving memory efficiency and</div><div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;performance when compared to most of the existing algorithms.</div><div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;</div><div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;## F14 VARIANTS</div><div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;</div><div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;The core hash table implementation has a pluggable storage strategy,</div><div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;with three policies provided:</div><div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;</div><div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;F14NodeMap stores values indirectly, calling malloc on each insert like</div><div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;std::unordered_map.  This implementation is the most memory efficient</div><div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;for medium and large keys.  It provides the same iterator and reference</div><div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;stability guarantees as the standard map while being faster and more</div><div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;memory efficient, so you can substitute F14NodeMap for std::unordered_map</div><div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;safely in production code.  F14&#39;s filtering substantially reduces</div><div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;indirection (and cache misses) when compared to std::unordered_map.</div><div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;</div><div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;F14ValueMap stores values inline, like google::dense_hash_map.</div><div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;Inline storage is the most memory efficient for small values, but for</div><div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;medium and large values it wastes space.  Because it can tolerate a much</div><div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;higher load factor, F14ValueMap is almost twice as memory efficient as</div><div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;dense_hash_map while also faster for most workloads.</div><div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;</div><div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;F14VectorMap keeps values packed in a contiguous array.  The main hash</div><div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;array stores 32-bit indexes into the value vector.  Compared to the</div><div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;existing internal implementations that use a similar strategy, F14 is</div><div class="line"><a name="l00043"></a><span class="lineno">   43</span>&#160;slower for simple keys and small or medium-sized tables (because of the</div><div class="line"><a name="l00044"></a><span class="lineno">   44</span>&#160;cost of bit mixing), faster for complex keys and large tables, and saves</div><div class="line"><a name="l00045"></a><span class="lineno">   45</span>&#160;about 16 bytes per entry on average.</div><div class="line"><a name="l00046"></a><span class="lineno">   46</span>&#160;</div><div class="line"><a name="l00047"></a><span class="lineno">   47</span>&#160;We also provide:</div><div class="line"><a name="l00048"></a><span class="lineno">   48</span>&#160;</div><div class="line"><a name="l00049"></a><span class="lineno">   49</span>&#160;F14FastMap inherits from either F14ValueMap or F14VectorMap depending</div><div class="line"><a name="l00050"></a><span class="lineno">   50</span>&#160;on entry size. When the key and mapped_type are less than 24 bytes, it</div><div class="line"><a name="l00051"></a><span class="lineno">   51</span>&#160;inherits from F14ValueMap. For medium and large entries, it inherits</div><div class="line"><a name="l00052"></a><span class="lineno">   52</span>&#160;from F14VectorMap. This strategy provides the best performance, while</div><div class="line"><a name="l00053"></a><span class="lineno">   53</span>&#160;also providing better memory efficiency than dense_hash_map or the other</div><div class="line"><a name="l00054"></a><span class="lineno">   54</span>&#160;hash tables in use at Facebook that don&#39;t individually allocate nodes.</div><div class="line"><a name="l00055"></a><span class="lineno">   55</span>&#160;</div><div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;## WHICH F14 VARIANT IS RIGHT FOR ME?</div><div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;</div><div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160;F14FastMap is a good default choice. If you care more about memory</div><div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;efficiency than performance, F14NodeMap is better for medium and</div><div class="line"><a name="l00060"></a><span class="lineno">   60</span>&#160;large entries.  F14NodeMap is the only F14 variant that doesn&#39;t move</div><div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;its elements, so in the rare case that you need reference stability you</div><div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160;should use it.</div><div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;</div><div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;## HETEROGENEOUS KEY TYPE WITH TRANSPARENT HASH AND EQUALITY</div><div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;</div><div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;In some cases it makes sense to define hash and key equality across</div><div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;types.  For example, StringPiece&#39;s hash and equality are capable of</div><div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;accepting std::string (because std::string is implicitly convertible</div><div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;to StringPiece).  If you mark the hash functor and key equality functor</div><div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;as _transparent_, then F14 will allow you to search the table directly</div><div class="line"><a name="l00071"></a><span class="lineno">   71</span>&#160;using any of the accepted key types without converting the key.</div><div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;</div><div class="line"><a name="l00073"></a><span class="lineno">   73</span>&#160;For example, using H =</div><div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;folly::transparent&lt;folly::hasher&lt;folly::StringPiece&gt;&gt; and</div><div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;E = folly::transparent&lt;std::equal_to&lt;folly::StringPiece&gt;&gt;, an</div><div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;F14FastSet&lt;std::string, H, E&gt; will allow you to use a StringPiece key</div><div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;without the need to construct a std::string.</div><div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;</div><div class="line"><a name="l00079"></a><span class="lineno">   79</span>&#160;Heterogeneous lookup and erase works for any key types that can be passed</div><div class="line"><a name="l00080"></a><span class="lineno">   80</span>&#160;to operator() on the hasher and key_equal functors.  For operations</div><div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;such as operator[] that might insert there is an additional constraint,</div><div class="line"><a name="l00082"></a><span class="lineno">   82</span>&#160;which is that the passed-in key must be explicitly convertible to the</div><div class="line"><a name="l00083"></a><span class="lineno">   83</span>&#160;table&#39;s key_type.  F14 maps understand all possible forms that can be</div><div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;used to construct the underlying std::pair&lt;key_type const, value_type),</div><div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;so heterogeneous keys can be used even with insert and emplace.</div><div class="line"><a name="l00086"></a><span class="lineno">   86</span>&#160;</div><div class="line"><a name="l00087"></a><span class="lineno">   87</span>&#160;## WHY CHUNKS?</div><div class="line"><a name="l00088"></a><span class="lineno">   88</span>&#160;</div><div class="line"><a name="l00089"></a><span class="lineno">   89</span>&#160;Assuming that you have a magic wand that lets you search all of the keys</div><div class="line"><a name="l00090"></a><span class="lineno">   90</span>&#160;in a chunk in a single step (our wand is called _mm_cmpeq_epi8), then</div><div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;using chunks fundamentally improves the load factor/collision tradeoff.</div><div class="line"><a name="l00092"></a><span class="lineno">   92</span>&#160;The cost is proportional only to the number of chunks visited to find</div><div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;the key.</div><div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;</div><div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;It&#39;s kind of like the birthday paradox in reverse.  In a room with 23</div><div class="line"><a name="l00096"></a><span class="lineno">   96</span>&#160;people there is a 50/50 chance that two of them have the same birthday</div><div class="line"><a name="l00097"></a><span class="lineno">   97</span>&#160;(overflowing a chunk with capacity 1), but the chance that 8 of them</div><div class="line"><a name="l00098"></a><span class="lineno">   98</span>&#160;were born in the same week (overflowing a chunk with capacity 7) is</div><div class="line"><a name="l00099"></a><span class="lineno">   99</span>&#160;very small.  Even though the chance of any two people being born in</div><div class="line"><a name="l00100"></a><span class="lineno">  100</span>&#160;the same week is higher (1/52 instead of 1/365), the larger number of</div><div class="line"><a name="l00101"></a><span class="lineno">  101</span>&#160;coincidences required means that the final probability is much lower</div><div class="line"><a name="l00102"></a><span class="lineno">  102</span>&#160;(less than 1 in a million). It would require 160 people to reach a 50/50</div><div class="line"><a name="l00103"></a><span class="lineno">  103</span>&#160;chance that 8 of them were born in the same week.</div><div class="line"><a name="l00104"></a><span class="lineno">  104</span>&#160;</div><div class="line"><a name="l00105"></a><span class="lineno">  105</span>&#160;## WHY PROBING?</div><div class="line"><a name="l00106"></a><span class="lineno">  106</span>&#160;</div><div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160;Chaining to a new chunk on collision is not very memory efficient,</div><div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;because the new chunk is almost certain to be under-filled.  We tried</div><div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160;chaining to individual entries, but that bloated the lookup code and</div><div class="line"><a name="l00110"></a><span class="lineno">  110</span>&#160;can&#39;t match the performance of a probing strategy.</div><div class="line"><a name="l00111"></a><span class="lineno">  111</span>&#160;</div><div class="line"><a name="l00112"></a><span class="lineno">  112</span>&#160;At our max load factor of 12/14, the expected probe length when searching</div><div class="line"><a name="l00113"></a><span class="lineno">  113</span>&#160;for an existing key (find hit) is 1.04, and fewer than 1% of keys are</div><div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;not found in one of the first 3 chunks.  When searching for a key that is</div><div class="line"><a name="l00115"></a><span class="lineno">  115</span>&#160;not in the map (find miss) the expected probe length at max load factor</div><div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;is 1.275 and the P99 probe length is 4.</div><div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;</div><div class="line"><a name="l00118"></a><span class="lineno">  118</span>&#160;## CHUNK OVERFLOW COUNTS: REFERENCE-COUNTED TOMBSTONES</div><div class="line"><a name="l00119"></a><span class="lineno">  119</span>&#160;</div><div class="line"><a name="l00120"></a><span class="lineno">  120</span>&#160;Hash tables with a complex probing strategy (quadratic or double-hashing)</div><div class="line"><a name="l00121"></a><span class="lineno">  121</span>&#160;typically use a tombstone on erase, because it is very difficult to</div><div class="line"><a name="l00122"></a><span class="lineno">  122</span>&#160;find the keys that might have been displaced by a full bucket (i.e.,</div><div class="line"><a name="l00123"></a><span class="lineno">  123</span>&#160;chunk in F14).  If the probing strategy allows only a small number of</div><div class="line"><a name="l00124"></a><span class="lineno">  124</span>&#160;potential destinations for a displaced key (linear probing, Robin Hood</div><div class="line"><a name="l00125"></a><span class="lineno">  125</span>&#160;hashing, or Cuckoo hashing), it is also an option to find a displaced key,</div><div class="line"><a name="l00126"></a><span class="lineno">  126</span>&#160;relocate it, and then recursively repair the new hole.</div><div class="line"><a name="l00127"></a><span class="lineno">  127</span>&#160;</div><div class="line"><a name="l00128"></a><span class="lineno">  128</span>&#160;Tombstones must be eventually reclaimed to deal with workloads that</div><div class="line"><a name="l00129"></a><span class="lineno">  129</span>&#160;continuously insert and erase.  google::dense_hash_map eventually triggers</div><div class="line"><a name="l00130"></a><span class="lineno">  130</span>&#160;a rehash in this case, for example.  Unfortunately, to avoid quadratic</div><div class="line"><a name="l00131"></a><span class="lineno">  131</span>&#160;behavior this rehash may have to halve the max load factor of the table,</div><div class="line"><a name="l00132"></a><span class="lineno">  132</span>&#160;resulting in a huge decrease in memory efficiency.</div><div class="line"><a name="l00133"></a><span class="lineno">  133</span>&#160;</div><div class="line"><a name="l00134"></a><span class="lineno">  134</span>&#160;Although most probing algorithms just keep probing until they find an</div><div class="line"><a name="l00135"></a><span class="lineno">  135</span>&#160;empty slot, probe lengths can be substantially reduced if you track</div><div class="line"><a name="l00136"></a><span class="lineno">  136</span>&#160;whether a bucket has actually rejected a key.  This &quot;overflow bit&quot;</div><div class="line"><a name="l00137"></a><span class="lineno">  137</span>&#160;is set when an attempt is made to place a key into the bucket but the</div><div class="line"><a name="l00138"></a><span class="lineno">  138</span>&#160;bucket was full.  (An especially unlucky key might have to try several</div><div class="line"><a name="l00139"></a><span class="lineno">  139</span>&#160;buckets, setting the overflow bit in each.)  Amble and Knuth describe an</div><div class="line"><a name="l00140"></a><span class="lineno">  140</span>&#160;overflow bit in the &quot;Further development&quot; section of &quot;Ordered hash tables&quot;</div><div class="line"><a name="l00141"></a><span class="lineno">  141</span>&#160;(https://academic.oup.com/comjnl/article/17/2/135/525363).</div><div class="line"><a name="l00142"></a><span class="lineno">  142</span>&#160;</div><div class="line"><a name="l00143"></a><span class="lineno">  143</span>&#160;The overflow bit subsumes the role of a tombstone, since a tombstone&#39;s</div><div class="line"><a name="l00144"></a><span class="lineno">  144</span>&#160;only effect is to cause a probe search to continue.  Unlike a tombstone,</div><div class="line"><a name="l00145"></a><span class="lineno">  145</span>&#160;however, the overflow bit is a property of the keys that were displaced</div><div class="line"><a name="l00146"></a><span class="lineno">  146</span>&#160;rather than the key that was erased.  It&#39;s only a small step to turn</div><div class="line"><a name="l00147"></a><span class="lineno">  147</span>&#160;this into a counter that records the number of displaced keys, and that</div><div class="line"><a name="l00148"></a><span class="lineno">  148</span>&#160;can be decremented on erase.  Overflow counts give us both an earlier</div><div class="line"><a name="l00149"></a><span class="lineno">  149</span>&#160;exit from probing and the effect of a reference-counted tombstone.</div><div class="line"><a name="l00150"></a><span class="lineno">  150</span>&#160;They automatically clean themselves up in a steady-state insert and</div><div class="line"><a name="l00151"></a><span class="lineno">  151</span>&#160;erase workload, giving us the upsides of double-hashing without the</div><div class="line"><a name="l00152"></a><span class="lineno">  152</span>&#160;normal downsides of tombstones.</div><div class="line"><a name="l00153"></a><span class="lineno">  153</span>&#160;</div><div class="line"><a name="l00154"></a><span class="lineno">  154</span>&#160;## HOW DOES VECTOR FILTERING WORK?</div><div class="line"><a name="l00155"></a><span class="lineno">  155</span>&#160;</div><div class="line"><a name="l00156"></a><span class="lineno">  156</span>&#160;F14 computes a secondary hash value for each key, which we call the key&#39;s</div><div class="line"><a name="l00157"></a><span class="lineno">  157</span>&#160;tag.  Tags are 1 byte: 7 bits of entropy with the top bit set.  The 14</div><div class="line"><a name="l00158"></a><span class="lineno">  158</span>&#160;tags are joined with 2 additional bytes of metadata to form a 16-byte</div><div class="line"><a name="l00159"></a><span class="lineno">  159</span>&#160;aligned __m128i at the beginning of the chunk.  When we&#39;re looking for a</div><div class="line"><a name="l00160"></a><span class="lineno">  160</span>&#160;key we can compare the needle&#39;s tag to all 14 tags in a chunk in parallel.</div><div class="line"><a name="l00161"></a><span class="lineno">  161</span>&#160;The result of the comparison is a bitmask that identifies only slots in</div><div class="line"><a name="l00162"></a><span class="lineno">  162</span>&#160;a chunk that might have a non-empty matching key.  Failing searches are</div><div class="line"><a name="l00163"></a><span class="lineno">  163</span>&#160;unlikely to perform any key comparisons, successful searches are likely</div><div class="line"><a name="l00164"></a><span class="lineno">  164</span>&#160;to perform exactly 1 comparison, and all of the resulting branches are</div><div class="line"><a name="l00165"></a><span class="lineno">  165</span>&#160;pretty predictable.</div><div class="line"><a name="l00166"></a><span class="lineno">  166</span>&#160;</div><div class="line"><a name="l00167"></a><span class="lineno">  167</span>&#160;The vector search is coded using SIMD intrinsics, SSE2 on x86_64 and</div><div class="line"><a name="l00168"></a><span class="lineno">  168</span>&#160;NEON on aarch64.  These instructions are a non-optional part of those</div><div class="line"><a name="l00169"></a><span class="lineno">  169</span>&#160;platforms (unlike later SIMD instruction sets like AVX2 or SVE), so no</div><div class="line"><a name="l00170"></a><span class="lineno">  170</span>&#160;special compilation flags are required.  The exact vector operations</div><div class="line"><a name="l00171"></a><span class="lineno">  171</span>&#160;performed differs between x86_64 and aarch64 because aarch64 lacks a</div><div class="line"><a name="l00172"></a><span class="lineno">  172</span>&#160;movemask instruction, but the F14 algorithm is the same.</div><div class="line"><a name="l00173"></a><span class="lineno">  173</span>&#160;</div><div class="line"><a name="l00174"></a><span class="lineno">  174</span>&#160;## WHAT ABOUT MEMORY OVERHEAD FOR SMALL TABLES?</div><div class="line"><a name="l00175"></a><span class="lineno">  175</span>&#160;</div><div class="line"><a name="l00176"></a><span class="lineno">  176</span>&#160;The F14 algorithm works well for large tables, because the tags can</div><div class="line"><a name="l00177"></a><span class="lineno">  177</span>&#160;fit in cache even when the keys and values can&#39;t.  Tiny hash tables are</div><div class="line"><a name="l00178"></a><span class="lineno">  178</span>&#160;by far the most numerous, however, so it&#39;s important that we minimize</div><div class="line"><a name="l00179"></a><span class="lineno">  179</span>&#160;the footprint when the table is empty or has only 1 or 2 elements.</div><div class="line"><a name="l00180"></a><span class="lineno">  180</span>&#160;Conveniently, tags cause keys to be densely packed into the bottom of</div><div class="line"><a name="l00181"></a><span class="lineno">  181</span>&#160;a chunk and filter all memory accesses to the portions of a chunk that</div><div class="line"><a name="l00182"></a><span class="lineno">  182</span>&#160;are not used.  That means that we can also support capacities that are</div><div class="line"><a name="l00183"></a><span class="lineno">  183</span>&#160;a fraction of 1 chunk with no change to any of the search and insertion</div><div class="line"><a name="l00184"></a><span class="lineno">  184</span>&#160;algorithms.  The only change required is in the check to see if a rehash</div><div class="line"><a name="l00185"></a><span class="lineno">  185</span>&#160;is required.  F14&#39;s first three capacities all use one chunk and one</div><div class="line"><a name="l00186"></a><span class="lineno">  186</span>&#160;16-byte metadata vector, but allocate space for 2, 6, and then 12 keys.</div><div class="line"><a name="l00187"></a><span class="lineno">  187</span>&#160;</div><div class="line"><a name="l00188"></a><span class="lineno">  188</span>&#160;## IS F14NODEMAP FULLY STANDARDS-COMPLIANT?</div><div class="line"><a name="l00189"></a><span class="lineno">  189</span>&#160;</div><div class="line"><a name="l00190"></a><span class="lineno">  190</span>&#160;No.  F14 does provide full support for stateful allocators, fancy</div><div class="line"><a name="l00191"></a><span class="lineno">  191</span>&#160;pointers, and as many parts of the C++ standard for unordered associative</div><div class="line"><a name="l00192"></a><span class="lineno">  192</span>&#160;containers as it can, but it is not fully standards-compliant.</div><div class="line"><a name="l00193"></a><span class="lineno">  193</span>&#160;</div><div class="line"><a name="l00194"></a><span class="lineno">  194</span>&#160;We don&#39;t know of a way to efficiently implement the full bucket API</div><div class="line"><a name="l00195"></a><span class="lineno">  195</span>&#160;in a table that uses double-hashed probing, in particular size_type</div><div class="line"><a name="l00196"></a><span class="lineno">  196</span>&#160;bucket(key_type const&amp;).  This function must compute the bucket index</div><div class="line"><a name="l00197"></a><span class="lineno">  197</span>&#160;for any key, even before it is inserted into the table.  That means</div><div class="line"><a name="l00198"></a><span class="lineno">  198</span>&#160;that a local_iterator range can&#39;t partition the key space by the chunk</div><div class="line"><a name="l00199"></a><span class="lineno">  199</span>&#160;that terminated probing during insert; the only partition choice with</div><div class="line"><a name="l00200"></a><span class="lineno">  200</span>&#160;reasonable locality would be the first-choice chunk.  The probe sequence</div><div class="line"><a name="l00201"></a><span class="lineno">  201</span>&#160;for a key in double-hashing depends on the key, not the first-choice</div><div class="line"><a name="l00202"></a><span class="lineno">  202</span>&#160;chunk, however, so it is infeasible to search for all of the displaced</div><div class="line"><a name="l00203"></a><span class="lineno">  203</span>&#160;keys given only their first-choice location.  We&#39;re unwilling to use an</div><div class="line"><a name="l00204"></a><span class="lineno">  204</span>&#160;inferior probing strategy or dedicate space to the required metadata just</div><div class="line"><a name="l00205"></a><span class="lineno">  205</span>&#160;to support the full bucket API.  Implementing the rest of the bucket API,</div><div class="line"><a name="l00206"></a><span class="lineno">  206</span>&#160;such as local_iterator begin(size_type), would not be difficult.</div><div class="line"><a name="l00207"></a><span class="lineno">  207</span>&#160;</div><div class="line"><a name="l00208"></a><span class="lineno">  208</span>&#160;F14 does not allow max_load_factor to be adjusted.  Probing tables</div><div class="line"><a name="l00209"></a><span class="lineno">  209</span>&#160;can&#39;t support load factors greater than 1, so the standards-required</div><div class="line"><a name="l00210"></a><span class="lineno">  210</span>&#160;ability to temporarily disable rehashing by temporarily setting a very</div><div class="line"><a name="l00211"></a><span class="lineno">  211</span>&#160;high max load factor just isn&#39;t possible.  We have also measured that</div><div class="line"><a name="l00212"></a><span class="lineno">  212</span>&#160;there is no performance advantage to forcing a low load factor, so it&#39;s</div><div class="line"><a name="l00213"></a><span class="lineno">  213</span>&#160;better just to omit the field and save space in every F14 instance.</div><div class="line"><a name="l00214"></a><span class="lineno">  214</span>&#160;This is part of the way we get empty maps down to 32 bytes.  The void</div><div class="line"><a name="l00215"></a><span class="lineno">  215</span>&#160;max_load_factor(float) method is still present, but does nothing.  We use</div><div class="line"><a name="l00216"></a><span class="lineno">  216</span>&#160;the default max_load_factor of 1.0f all of the time, adjusting the value</div><div class="line"><a name="l00217"></a><span class="lineno">  217</span>&#160;returned from size_type bucket_count() so that the externally-visible</div><div class="line"><a name="l00218"></a><span class="lineno">  218</span>&#160;load factor reaches 1 just as the actual internal load factor reaches</div><div class="line"><a name="l00219"></a><span class="lineno">  219</span>&#160;our threshold of 12/14.</div><div class="line"><a name="l00220"></a><span class="lineno">  220</span>&#160;</div><div class="line"><a name="l00221"></a><span class="lineno">  221</span>&#160;The standard requires that a hash table be iterable in O(size()) time</div><div class="line"><a name="l00222"></a><span class="lineno">  222</span>&#160;regardless of its load factor (rather than O(bucket_count()).  That means</div><div class="line"><a name="l00223"></a><span class="lineno">  223</span>&#160;if you insert 1 million keys then erase all but 10, iteration should</div><div class="line"><a name="l00224"></a><span class="lineno">  224</span>&#160;be O(10).  For std::unordered_map the cost of supporting this scenario</div><div class="line"><a name="l00225"></a><span class="lineno">  225</span>&#160;is an extra level of indirection in every read and every write, which is</div><div class="line"><a name="l00226"></a><span class="lineno">  226</span>&#160;part of why we can improve substantially on its performance.  Low load</div><div class="line"><a name="l00227"></a><span class="lineno">  227</span>&#160;factor iteration occurs in practice when erasing keys during iteration</div><div class="line"><a name="l00228"></a><span class="lineno">  228</span>&#160;(for example by repeatedly calling map.erase(map.begin())), so we provide</div><div class="line"><a name="l00229"></a><span class="lineno">  229</span>&#160;the weaker guarantee that iteration is O(size()) after erasing any prefix</div><div class="line"><a name="l00230"></a><span class="lineno">  230</span>&#160;of the iteration order.  F14VectorMap doesn&#39;t have this problem.</div><div class="line"><a name="l00231"></a><span class="lineno">  231</span>&#160;</div><div class="line"><a name="l00232"></a><span class="lineno">  232</span>&#160;The standard requires that clear() be O(size()), which has the practical</div><div class="line"><a name="l00233"></a><span class="lineno">  233</span>&#160;effect of prohibiting a change to bucket_count.  F14 deallocates</div><div class="line"><a name="l00234"></a><span class="lineno">  234</span>&#160;all memory during clear() if it has space for more than 100 keys, to</div><div class="line"><a name="l00235"></a><span class="lineno">  235</span>&#160;avoid leaving a large table that will be expensive to iterate (see the</div><div class="line"><a name="l00236"></a><span class="lineno">  236</span>&#160;previous paragraph).  google::dense_hash_map works around this tradeoff</div><div class="line"><a name="l00237"></a><span class="lineno">  237</span>&#160;by providing both clear() and clear_no_resize(); we could do something</div><div class="line"><a name="l00238"></a><span class="lineno">  238</span>&#160;similar.</div><div class="line"><a name="l00239"></a><span class="lineno">  239</span>&#160;</div><div class="line"><a name="l00240"></a><span class="lineno">  240</span>&#160;As stated above, F14NodeMap and F14NodeSet are the only F14 variants</div><div class="line"><a name="l00241"></a><span class="lineno">  241</span>&#160;that provides reference stability.  When running under ASAN the other</div><div class="line"><a name="l00242"></a><span class="lineno">  242</span>&#160;storage policies will probabilistically perform extra rehashes, which</div><div class="line"><a name="l00243"></a><span class="lineno">  243</span>&#160;makes it likely that reference stability problems will be found by the</div><div class="line"><a name="l00244"></a><span class="lineno">  244</span>&#160;address sanitizer.</div><div class="line"><a name="l00245"></a><span class="lineno">  245</span>&#160;</div><div class="line"><a name="l00246"></a><span class="lineno">  246</span>&#160;An additional subtlety for hash tables that don&#39;t provide reference</div><div class="line"><a name="l00247"></a><span class="lineno">  247</span>&#160;stability is whether they rehash before evaluating the arguments passed</div><div class="line"><a name="l00248"></a><span class="lineno">  248</span>&#160;to insert().  F14 tables may rehash before evaluating the arguments</div><div class="line"><a name="l00249"></a><span class="lineno">  249</span>&#160;to a method that causes an insertion, so it&#39;s not safe to write</div><div class="line"><a name="l00250"></a><span class="lineno">  250</span>&#160;something like `map.insert(k2, map[k1])` with F14FastMap, F14ValueMap,</div><div class="line"><a name="l00251"></a><span class="lineno">  251</span>&#160;or F14VectorMap.  This behavior matches google::dense_hash_map and the</div><div class="line"><a name="l00252"></a><span class="lineno">  252</span>&#160;excellent absl::flat_hash_map.</div><div class="line"><a name="l00253"></a><span class="lineno">  253</span>&#160;</div><div class="line"><a name="l00254"></a><span class="lineno">  254</span>&#160;F14NodeMap does not currently support the C++17 node API, but it could</div><div class="line"><a name="l00255"></a><span class="lineno">  255</span>&#160;be trivially added.</div><div class="line"><a name="l00256"></a><span class="lineno">  256</span>&#160;</div><div class="line"><a name="l00257"></a><span class="lineno">  257</span>&#160;* Nathan Bronson -- &lt;ngbronson@fb.com&gt;</div><div class="line"><a name="l00258"></a><span class="lineno">  258</span>&#160;* Xiao Shi -- &lt;xshi@fb.com&gt;</div></div><!-- fragment --></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="F14_8md.html">F14.md</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.11 </li>
  </ul>
</div>
</body>
</html>
